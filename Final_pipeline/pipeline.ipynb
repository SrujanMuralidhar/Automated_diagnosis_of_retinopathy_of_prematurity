{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xerom\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.20 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
    "from skimage import img_as_float\n",
    "from skimage.filters import gabor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import unet_model\n",
    "import cv2\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"pipeline-output\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (512, 512)\n",
    "path = r\"C:\\Users\\xerom\\Documents\\CAPSTONE\\Classification\\Data\\Stage 3\\2.png\"\n",
    "original_image = cv2.imread(path)\n",
    "original_image = cv2.resize(original_image, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "original_image_path = os.path.join(output_folder, \"original_image.png\")\n",
    "cv2.imwrite(original_image_path,original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Non-Temporal Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor Filter - used for Ridge Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enhance_edges(image):\n",
    "    image = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    image = np.uint8(image)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    # Convert the image to float for Sato filter\n",
    "    image_float = img_as_float(image)\n",
    "    # Define the structuring element (kernel) for morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (50, 50))\n",
    "    # Apply the Top-Hat transformation\n",
    "    top_hat = cv2.morphologyEx(image, cv2.MORPH_TOPHAT, kernel) \n",
    "    # Apply the Bottom-Hat transformation\n",
    "    bottom_hat = cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Enhance the image by adding Top-Hat and subtracting Bottom-Hat\n",
    "    enhanced_image = cv2.add(image, top_hat)   # Add Top-Hat to enhance bright regions\n",
    "    enhanced_image = cv2.subtract(enhanced_image, bottom_hat)  # Subtract Bottom-Hat to enhance dark regions\n",
    "    \n",
    "    return enhanced_image\n",
    "\n",
    "def gab(image):\n",
    "    img = enhance_edges(image)\n",
    "    filt_real, filt_imaginary = gabor(img,1/4,30,sigma_x=1,sigma_y=1)\n",
    "    # img = Image.fromarray(filt_real)\n",
    "    return filt_real\n",
    "\n",
    "def refine_edges(image):\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    center = (int(image.shape[1] / 2), int(image.shape[0] / 2))\n",
    "    radius = image.shape[1]//2 - 1\n",
    "    cv2.circle(mask, center, radius, 255, -1)\n",
    "    result = cv2.bitwise_and(mask,image)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gabor Filter\n",
    "\n",
    "gab_image = gab(original_image)\n",
    "gab_image = refine_edges(gab_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gabor image saved at pipeline-output\\gabor_image.png\n"
     ]
    }
   ],
   "source": [
    "gab_image_path = os.path.join(output_folder, \"gabor_image.png\")\n",
    "cv2.imwrite(gab_image_path,gab_image)\n",
    "print(f\"Gabor image saved at {gab_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Adaptive Siigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_correction(image, k=10, x0=0.5):\n",
    "    # Normalize the image\n",
    "    normalized_img = image / 255.0\n",
    "    # Apply the sigmoid function\n",
    "    sigmoid_img = 1 / (1 + np.exp(-k * (normalized_img - x0)))\n",
    "    # Scale back to original range\n",
    "    corrected_img = (sigmoid_img * 255).astype(np.uint8)\n",
    "    return corrected_img\n",
    "\n",
    "\n",
    "def adaptive_sigmoid(image):\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    center = (int(image.shape[1] / 2), int(image.shape[0] / 2))\n",
    "    radius = image.shape[1]//2\n",
    "    cv2.circle(mask, center, radius, 255, -1)\n",
    "    hist = cv2.calcHist([image], [1], mask, [256], [0, 256])\n",
    "    # Calculate cumulative distribution function (CDF)\n",
    "    cdf = hist.cumsum()\n",
    "    # Normalize CDF\n",
    "    cdf_normalized = cdf * hist.max() / cdf.max()\n",
    "    # Find the intensity level where CDF reaches 50% of the total pixel count\n",
    "    total_pixels = cdf[-1]\n",
    "    x_0 = np.searchsorted(cdf, total_pixels * 0.5)/255\n",
    "    k = 15\n",
    "    sig = sigmoid_correction(image,k,x_0)\n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_sigmoid_image = adaptive_sigmoid(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_sigmoid_image_path = os.path.join(output_folder, \"adaptive_sigmoid_image.png\")\n",
    "cv2.imwrite(adaptive_sigmoid_image_path,adaptive_sigmoid_image)\n",
    "print(f\"Adaptive Sigmoid image saved at {adaptive_sigmoid_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Blood Vessel and Ridge Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse mask for saving\n",
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)  # (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  # (512, 512, 3)\n",
    "    return mask\n",
    "\n",
    "# Load models\n",
    "bv_model = unet_model.build_unet()\n",
    "bv_model.load_state_dict(torch.load(r'Models\\bv_no_enhancement.pth', map_location=device))\n",
    "# bv_model = bv_model.cuda()  # Load model on GPU if available\n",
    "bv_model.eval()\n",
    "\n",
    "ridge_model = unet_model.build_unet()\n",
    "ridge_model.load_state_dict(torch.load(r'Models\\gabor_ridge_aug.pth', map_location=device))\n",
    "# ridge_model = ridge_model.cuda()  # Load model on GPU if available\n",
    "ridge_model.eval()\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load and resize the image\n",
    "def load_image(image_path):\n",
    "    if image_path.endswith(('.png', '.jpg', '.jpeg', '.tif')):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = image.resize((512, 512))\n",
    "        image = transform(image).unsqueeze(0).to(device)  # Ensure tensor is on GPU\n",
    "        return image\n",
    "    \n",
    "# Get mask prediction\n",
    "def get_prediction(model, image_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output[0].cpu().numpy()  # Move to CPU for further processing\n",
    "        output = np.squeeze(output, axis=0)\n",
    "        output = output > 0.5\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Superpose masks on the original image with sigmoid correction\n",
    "def superpose_masks(original_image, bv_mask, ridge_mask):\n",
    "    # Convert original image to NumPy\n",
    "    original_image_np = np.array(original_image)\n",
    "    \n",
    "    # Apply sigmoid correction to the original image\n",
    "    corrected_image_np = adaptive_sigmoid(original_image_np)\n",
    "    \n",
    "    # Initialize the combined mask with 3 channels (for RGB)\n",
    "    combined_mask = np.zeros((bv_mask.shape[0], bv_mask.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # COLOR coding for the MASKS\n",
    "    #-----------------------------------------------------------------------------\n",
    "    combined_mask[bv_mask == 1] = [179, 2, 2]  \n",
    "    combined_mask[ridge_mask == 1] = [25, 10, 242]  \n",
    "    #------------------------------------------------------------------------------\n",
    "    # Superpose the masks on the sigmoid-corrected original image\n",
    "    superposed_image = corrected_image_np.copy()\n",
    "    mask_indices = combined_mask > 0\n",
    "    superposed_image[mask_indices] = combined_mask[mask_indices]\n",
    "    \n",
    "    return Image.fromarray(superposed_image)\n",
    "\n",
    "# Process a single image\n",
    "def process_single_image(bv_image_path, ridge_image_path, output_path):\n",
    "    # Load the images and resize if necessary\n",
    "    bv_image_tensor = load_image(bv_image_path)\n",
    "    ridge_image_tensor = load_image(ridge_image_path)\n",
    "    original_image = Image.open(bv_image_path).convert('RGB').resize((512, 512))\n",
    "\n",
    "    # Generate masks\n",
    "    bv_mask = get_prediction(bv_model, bv_image_tensor)\n",
    "    ridge_mask = get_prediction(ridge_model, ridge_image_tensor)\n",
    "\n",
    "    # Superpose the masks onto the original image\n",
    "    superposed_image = superpose_masks(original_image, bv_mask, ridge_mask)\n",
    "\n",
    "    # Save the output image\n",
    "    superposed_image.save(output_path)\n",
    "    print(f\"Superposed image saved at {output_path}\")\n",
    "\n",
    "# Single image input and output paths\n",
    "bv_image_path = path\n",
    "ridge_image_path = r'pipeline-output\\gabor_image.png'\n",
    "output_path = r'pipeline-output\\superposed_image.png'\n",
    "\n",
    "# Process the single image\n",
    "process_single_image(bv_image_path, ridge_image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the Superposed image as input to Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0 -> No RoP\n",
    "1 -> RoP\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 ## Either RoP/ No RoP\n",
    "\n",
    "# Load ResNet50 model\n",
    "model = models.resnet50()\n",
    "model.fc = torch.nn.Linear(model.fc.in_features,NUM_CLASSES )\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load your trained weights if needed\n",
    "model.load_state_dict(torch.load('Models\\ResNet50_Rop_NoRop.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Define transformations for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the model's expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Use ImageNet's mean and std\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image.to(device)\n",
    "\n",
    "# Prediction function\n",
    "def predict(image_path, model):\n",
    "    image = load_image(image_path)\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)  # Get the class index with highest score\n",
    "    return predicted.item()\n",
    "\n",
    "# Test on a single image\n",
    "image_path = r'pipeline-output\\superposed_image.png'\n",
    "predicted_class = predict(image_path, model)\n",
    "print(f'Predicted class index: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Predicted Class == 0 -> NO RoP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take PMA as input and if PMA > 40 then classify it as FVR else classify it as TAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Predicted Class == 1 -> RoP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0 -> Stage 1\n",
    "1 -> Stage 2\n",
    "2 -> Stage 3\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "NUM_CLASSES = 3 # Stage1, Stage2, Stage3\n",
    "\n",
    "# Load ResNet50 model\n",
    "model = models.resnet50()\n",
    "model.fc = torch.nn.Linear(model.fc.in_features,NUM_CLASSES )\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load your trained weights if needed\n",
    "model.load_state_dict(torch.load('Models\\ResNet50_3Stages.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Define transformations for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the model's expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Use ImageNet's mean and std\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image.to(device)\n",
    "\n",
    "# Prediction function\n",
    "def predict(image_path, model):\n",
    "    image = load_image(image_path)\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)  # Get the class index with highest score\n",
    "    return predicted.item()\n",
    "\n",
    "# Test on a single image\n",
    "image_path = r'pipeline-output\\superposed_image.png'\n",
    "predicted_class = predict(image_path, model)\n",
    "print(f'Predicted class index: {predicted_class}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
